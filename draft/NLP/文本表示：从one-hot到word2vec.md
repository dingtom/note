

# one-hot 

用一个很长的向量来表示一个词，向量长度为词典的大小N，每个向量只有一个维度为1，其余维度全部为0，为1的位置表示该词语在词典的位置。

##  one-hot 存在哪些问题

- 维度灾难：容易**受维数灾难的困扰**，每个词语的维度就是语料库字典的长度；
- **离散、稀疏问题**：因为 one-Hot 中，句子向量，如果词出现则为1，没出现则为0，但是由于维度远大于句子长度，所以句子中的1远小于0的个数；
- **维度鸿沟问题**：词语的编码往往是随机的，导致不能很好地刻画**词与词之间的相似性**。



# TF-IDF

TF-IDF 是一种统计方法，用以评估句子中的**某一个词（字）对于整个文档的重要程度**。如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类；

- 对于 句子中的某一个词（字）随着其在整个句子中的出现次数的增加，其重要性也随着增加；（正比关系）【体现词在句子中频繁性】
- 对于 句子中的某一个词（字）随着其在整个文档中的出现频率的增加，其重要性也随着减少；（反比关系）【体现词在文档中的唯一性】

### 2.4 TF-IDF 的计算公式是什么？

- 词频 （Term Frequency，TF）：体现 词 在 句子 中出现的频率；
  - 问题：
    - 当一个句子长度的增加，句子中 每一个 出现的次数 也会随之增加，导致该值容易偏向长句子；
    - 解决方法：
      - 需要做归一化（词频除以句子总字数）
  - 公式







### TF-IDF 的优点

- 容易理解；
- 容易实现；

### TF-IDF 的缺点

其简单结构并没有考虑词语的语义信息，无法处理一词多义与一义多词的情况。

### TF-IDF 的应用

- 搜索引擎；
- 关键词提取；
- 文本相似性；
- 文本摘要













































### 词嵌入



### ELMo：语境问题



![Image](https://mmbiz.qpic.cn/mmbiz_png/7PuqRWWU6zO4ynwzyItMQBcv4Ph7uS1dFribm6N4iaFVZNqWncksIBSWdI3008ZwCLwRwFllNmaZkf3b8Fgia1mOg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

ELMo是NLP社区对**一词多义**问题的回应——相同的词在不同的语境中有不同的含义。ELMo 没有对每个单词使用固定的词嵌入，而是在为每个词分配词嵌入之前，查看整个句子，融合上下文信息。它使用在特定任务上经过训练的双向 LSTM 来创建这些词嵌入。

![quicker_611c4dd0-5e57-4158-92f2-c8964f64ab2f.png](https://s2.loli.net/2022/03/16/yfrvJ6O4ZDiXYBP.png)

ELMo 通过将隐藏层状态（以及初始化的词嵌入）以某种方式（向量拼接之后加权求和）结合在一起，实现了带有语境化的词嵌入。

![quicker_720d91b2-7bf7-4710-b98d-2ce9a41d02fc.png](https://s2.loli.net/2022/03/16/aqBsK7CUj1fGF5n.png)



### ULM-FiT：NLP 领域的迁移学习

ULM-FiT 提出了一些方法来有效地利用模型在预训练期间学习到的东西 - 这些东西不仅仅是词嵌入，还有语境化的词嵌入。ULM-FiT 提出了一个语言模型和一套流程，可以有效地为各种任务微调这个语言模型。

#### OpenAI的GPT

OpenAI的GPT扩展了ULMFiT和ELMo引入的预训练和微调方法。GPT本质上用基于转换的体系结构取代了基于lstm的语言建模体系结构。

GPT模型可以微调到文档分类之外的多个NLP任务，如常识推理、语义相似性和阅读理解。

GPT还强调了Transformer框架的重要性，它具有更简单的体系结构，并且比基于lstm的模型训练得更快。它还能够通过使用注意力机制来学习数据中的复杂模式。